AI ILE ÇALIŞIRKEN ÖĞRENDIĞIM BAZI KISIMLAR VE ONLARIN NOT HALINDE ÇIKMIŞLARI

TF-IDF
term frequency(count) - ınverse document frequency(how many documents this word appears in)
kelime sıklığı - terimin ne kadar ayırt edici olduğu

bir kelimenin belgede ne kadar önemli olduğunu ölçmeye yarar

intuitive idea

       term frequency
-----------------------------  = TF-IDF
 inverse document frequency

TF(t, d) = kelimenin belge d içinde geçme sayısı / belge d'deki toplam kelime sayısı
IDF(t) = log(Toplam belge sayısı / (Kelimeyi içeren belge sayısı + 1))+1  smooth idf 

Stopword’leri (the, is, and) otomatik olarak baskılar çünkü onlar her belgede vardır → IDF’leri düşüktür.
Özel ve az geçen kelimeleri öne çıkarır → IDF’leri yüksektir.
Hem belgeye özel hem belgeye göre ayırt edici kelimeleri tespit eder.

tfidf(t,d)= tf(t,d) x idf(t)

----------------------------TERM FREQUENCY (TF)--------------------------------------------------------

CoutVectorizerda çağırdığımız şeyde gelen budur

pythonda nasıl kullanılır:::

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf=TfidfVectorizer(
     bu kısma parametreler gelebilir  )  burada modeli oluşturduk we create the model

^^^^örnek^^^
tfidf = TfidfVectorizer(
    stop_words='english',             # İngilizce stopword'leri çıkarır
    tokenizer=my_tokenizer,           # Kendi yazdığın tokenizer fonksiyonunu kullanır
    strip_accents='unicode',          # Aksanlı harfleri düzleştirir ("ç" -> "c", "ş" -> "s")
    max_df=0.9,                       # %90'dan fazla dokümanda geçen kelimeleri dışlar
    min_df=5,                         # En az 5 dokümanda geçen kelimeleri alır
    ngram_range=(1, 2)                # Unigram ve bigram kullanır
)
^^^^^^^^^^^^^^^^

Xtrain = tfidf.fit_transform(train_texts)
Xtest = tfidf.transform(test_texts)


------------------------------------------------------------------------------------------------------------------------------------



eucledian distanceda mesafe bulacağız
mesafe similaritynin tersi olarak düşüneceğiz
eğer iki vektör uzak taraflara düşerse bu iki vektörün benzerliğinin az olduğunu gösterir = dissimilar

similarity e bakmanın başka bir yolu da aralarındaki açıya bakmak
aralarındaki açı genelde cosine
kosinüs fonksiyonundaki değerler bize vektörlerin ne kadar benzediğini gösterir
1 __              __   bu kısımlarda tamamen aynı olacak şekilde oluyor
   _           _       yani most similar
    _         _
-----_-------_-------- 0
      _     _
       _   _
-1      __            bu kısımda ise tamamen farklı anlamına geliyor çünkü aralarındaki açı tamamen 180


cosine distance = 1 - cosine similarity

cosine distance eucladian distancea göre daha mantıklı bir seçim oluyor çünkü eucladian distance kelimelerin
çoğunluğuna göre davrandığı için az x kelimesi ile az y kelimesi birbirinden tamamen farklı olsa bile eucladian
distanceda similaritysi yüksek çıkabiliyor fakat cosine distanceda x ve y tamamen birbirinden farklı olduğu için
aralarındaki açı 180 derece oluyor . cosinde distance daha güvenilir bir çözüm yolu


------------------------------------------------------------------------------------------------------------------------------------

-------------------------------------------- STEMMING AND LEMMATIZATION ------------------------------------------------------------

import nltk
from nltk.stem import PorterStemmer
porter = PorterStemmer()

print(porter.stem("walk"))  # walk
print(porter.stem("walking"))  #walk
print(porter.stem("walked"))    #walk
print(porter.stem("walking"))   #walk
print(porter.stem("replacement"))  #replac !!!! burada stemmingin yanlış da karar verdiğini gördük

sentence = "Lemmatization is more sophisticated than stemming".split()

print(sentence)  # split methodu ile bütün kelimeler ayrıldı

for token in sentence: (
print(porter.stem(token), end=" "),  # burada print içindeki porter.stem(token) ile de bütün
    # tokenların stemming işlemi yapıldı
)


print(porter.stem("\nunnecessary"))  # burada sonu y ile biteni i ile yazdırıyor "unnecessari"

from nltk.stem import WordNetLemmatizer

from nltk.corpus import wordnet
lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize("as"))
print(lemmatizer.lemmatize("walking"))
print(lemmatizer.lemmatize("walking" ,pos = wordnet.VERB))   #bu kısımda walkingin verb olarak algıladı
                                                                    # ve sonundaki ing takısını kaldırmış oldu

print(lemmatizer.lemmatize("going"))

print(lemmatizer.lemmatize("going" ,pos = wordnet.VERB))    #go gelmiş olacak

lemmatizer.lemmatize("going", pos=wordnet.VERB)   # verb olduğunu belirttik ve go olarak aldı
lemmatizer.lemmatize("ran", pos=wordnet.VERB)     # verb olarak geçmiş halde olduğunu bildi ve run olarak aldı
porter.stem("mice")             #  stemde herhangi bir değişiklik yapmadı mice olarak kalır
lemmatizer.lemmatize("mice")    #  lemmatizerda miceın mousedan geldiğini bildiği için mouse olarak aldı
porter.stem("was")              #  stemde tekrardan ne olduğunu bilmediği için wa olarak aldı
lemmatizer.lemmatize("was", pos=wordnet.VERB)       # burada verb olarak verdik geçmiş zaman o yüzden be aldı
porter.stem("is")           #  stemde is olur
lemmatizer.lemmatize("is", pos=wordnet.VERB)            # tekrardan be olur
porter.stem("better")           #  stemde better olarak bırakır kısaltma yapmaz
lemmatizer.lemmatize("better", pos=wordnet.ADJ)         # burada kelimenin türü adj dedik ve o yüzden good
                                                              # olarak aldı kelimenin kökünü biliyor



def get_wordnet_pos(treebank_tag):
  if treebank_tag.startswith('J'):
    return wordnet.ADJ
  elif treebank_tag.startswith('V'):
    return wordnet.VERB                                 # burada fonskiyon tanımlamış olduk bu fonksiyon bizim
  elif treebank_tag.startswith('N'):                    # vereceğimiz harfe göre wordnet türünü belirleyecek
    return wordnet.NOUN                                 # bir nevi bir kısaltma yapmış olduk
  elif treebank_tag.startswith('R'):
    return wordnet.ADV
  else:
    return wordnet.NOUN

sentence = "Donald Trump has a devoted following".split()


words_and_tags = nltk.pos_tag(sentence)
print(words_and_tags)






